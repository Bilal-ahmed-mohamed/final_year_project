class FakeNewsDetector:
    def __init__(self):
        self.models = {}
        self.vectorizer = None
        self.le = LabelEncoder()
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        self.sia = SentimentIntensityAnalyzer()
        self.scaler = StandardScaler()
        self.imputer = SimpleImputer(strategy='mean')
        self.min_max_scaler = MinMaxScaler()
        self.n_text_features = None
        
    def preprocess_text(self, text):
        print("preprocessing the text...")
        """Clean and preprocess text data"""
        text = text.lower()
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        text = re.sub(r'\S+@\S+', '', text)
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        tokens = word_tokenize(text)
        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]
        return ' '.join(tokens)
    
    def extract_linguistic_features(self, text):
        print("linguistic functionality...")
        features = {}
        try:
            features['word_count'] = len(text.split())
            features['char_count'] = len(text)
            words = text.split()
            features['avg_word_length'] = np.mean([len(word) for word in words]) if words else 0
            features['unique_words'] = len(set(words))
            features['unique_words_ratio'] = features['unique_words'] / features['word_count'] if features['word_count'] > 0 else 0
            sentences = sent_tokenize(text)
            features['sentence_count'] = len(sentences)
            features['avg_sentence_length'] = np.mean([len(sent.split()) for sent in sentences]) if sentences else 0
            features['avg_sentence_char_length'] = np.mean([len(sent) for sent in sentences]) if sentences else 0
            punct_counts = Counter(c for c in text if c in string.punctuation)
            features['exclamation_count'] = punct_counts['!']
            features['question_count'] = punct_counts['?']
            features['punctuation_ratio'] = sum(punct_counts.values()) / len(text) if len(text) > 0 else 0
        except Exception as e:
            print(f"Error in extract_linguistic_features: {str(e)}")
            # Fill with default values if there's an error
            features = {k: 0 for k in [
                'word_count', 'char_count', 'avg_word_length', 'unique_words',
                'unique_words_ratio', 'sentence_count', 'avg_sentence_length',
                'avg_sentence_char_length', 'exclamation_count', 'question_count',
                'punctuation_ratio'
            ]}
        return features    
    def extract_sentiment_features(self, text):
        features = {}
        vader_scores = self.sia.polarity_scores(text)
        features.update({f'vader_{k}': v for k, v in vader_scores.items()})
        blob = TextBlob(text)
        features['textblob_polarity'] = blob.sentiment.polarity
        features['textblob_subjectivity'] = blob.sentiment.subjectivity
        return features
    
    def extract_structural_features(self, text):
        features = {}
        words = text.split()
        features['caps_count'] = sum(1 for word in words if word.isupper())
        features['caps_ratio'] = features['caps_count'] / len(words) if words else 0
        paragraphs = text.split('\n\n')
        features['paragraph_count'] = len(paragraphs)
        features['avg_paragraph_length'] = np.mean([len(p.split()) for p in paragraphs]) if paragraphs else 0
        return features
    
    def extract_pos_features(self, text):
        features = {}
        doc = nlp(text)
        pos_counts = Counter(token.pos_ for token in doc)
        for pos, count in pos_counts.items():
            features[f'pos_{pos.lower()}'] = count
        ner_counts = Counter(ent.label_ for ent in doc.ents)
        for ner, count in ner_counts.items():
            features[f'ner_{ner.lower()}'] = count
        total_tokens = len(doc)
        if total_tokens > 0:
            features['noun_ratio'] = pos_counts['NOUN'] / total_tokens if 'NOUN' in pos_counts else 0
            features['verb_ratio'] = pos_counts['VERB'] / total_tokens if 'VERB' in pos_counts else 0
            features['adj_ratio'] = pos_counts['ADJ'] / total_tokens if 'ADJ' in pos_counts else 0
        return features
    
    def extract_readability_features(self, text):
        features = {}
        words = text.split()
        sentences = sent_tokenize(text)
        word_count = len(words)
        sentence_count = len(sentences)
        if sentence_count > 0 and word_count > 0:
            features['avg_words_per_sentence'] = word_count / sentence_count
            syllable_count = sum([self.count_syllables(word) for word in words])
            features['flesch_reading_ease'] = 206.835 - 1.015 * (word_count / sentence_count) - 84.6 * (syllable_count / word_count)
            complex_words = sum(1 for word in words if self.count_syllables(word) >= 3)
            features['gunning_fog'] = 0.4 * ((word_count / sentence_count) + 100 * (complex_words / word_count))
        return features
    
    def count_syllables(self, word):
        word = word.lower()
        count = 0
        vowels = 'aeiouy'
        if word[0] in vowels:
            count += 1
        for index in range(1, len(word)):
            if word[index] in vowels and word[index - 1] not in vowels:
                count += 1
        if word.endswith('e'):
            count -= 1
        if count == 0:
            count += 1
        return count
    
    def prepare_data(self, df, text_column, label_column):
        print("Starting data preparation and feature engineering...")
    
        # Clean and standardize label types
        df[label_column] = df[label_column].astype(str).str.strip()
        
        # Remove rows with NaN or non-string text
        df = df.dropna(subset=[text_column])
        df = df[df[text_column].apply(lambda x: isinstance(x, str))]
        
        df['processed_text'] = df[text_column].apply(self.preprocess_text)
    
        # Extract features
        linguistic_features = pd.DataFrame(df['processed_text'].apply(self.extract_linguistic_features).tolist())
        sentiment_features = pd.DataFrame(df['processed_text'].apply(self.extract_sentiment_features).tolist())
        structural_features = pd.DataFrame(df['processed_text'].apply(self.extract_structural_features).tolist())
        pos_features = pd.DataFrame(df['processed_text'].apply(self.extract_pos_features).tolist())
        readability_features = pd.DataFrame(df['processed_text'].apply(self.extract_readability_features).tolist())

    
        # TF-IDF features
        if self.vectorizer is None:
            self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))
            text_features = self.vectorizer.fit_transform(df['processed_text'])
        else:
            text_features = self.vectorizer.transform(df['processed_text'])
        
        # Store number of text features
        self.n_text_features = text_features.shape[1]
    
        # Combine all non-text features
        self.other_features = [
            linguistic_features,
            sentiment_features,
            structural_features,
            pos_features,
            readability_features
        ]
        all_features = pd.concat(self.other_features, axis=1)

         # Add advanced feature selection
        from sklearn.feature_selection import SelectFromModel
    
        # Store feature names and their order
        self.feature_names = all_features.columns.tolist()
    
        # Handle NaN values and scale features
        all_features = pd.DataFrame(self.imputer.fit_transform(all_features), columns=all_features.columns)
        other_features_array = all_features.values
        scaled_features = self.scaler.fit_transform(other_features_array)
    
        # Combine all features
        self.feature_matrix = np.hstack((text_features.toarray(), scaled_features))
    
        # Transform labels
        labels = self.le.fit_transform(df[label_column])
        
        return self.feature_matrix, labels   
    def train_models(self, X_train, y_train, X_test):
        print("Starting model training...")
    
        # Split features into text and other features
        n_text_features = self.vectorizer.get_feature_names_out().shape[0]
        X_train_text = X_train[:, :n_text_features]
        X_train_other = X_train[:, n_text_features:]
    
        # Handle any potential NaN values in the features
        X_train_other = self.imputer.transform(X_train_other)
        
        # Scale the non-text features
        X_train_other_scaled = self.scaler.transform(X_train_other)
    
        # Combine features
        X_train_combined = np.hstack((X_train_text, X_train_other_scaled))
    
        # Check for any remaining NaN values
        if np.isnan(X_train_combined).any():
            print("Warning: NaN values still present after preprocessing")
            # Replace any remaining NaN values with 0
            X_train_combined = np.nan_to_num(X_train_combined)
            
        # Advanced model configurations with hyperparameter tuning
        model_configs = {
            'random_forest': {
                'model': RandomForestClassifier(random_state=42),
                'params': {
                    'n_estimators': [100, 200, 300],
                    'max_depth': [None, 10, 20, 30],
                    'min_samples_split': [2, 5, 10],
                    'class_weight': ['balanced', None]
                }
            },
            'logistic_regression': {
                'model': LogisticRegression(max_iter=1000, random_state=42),
                'params': {
                    'C': [0.1, 1, 10],
                    'penalty': ['l1', 'l2'],
                    'solver': ['liblinear'],
                    'class_weight': ['balanced', None]
                }
            },
            'svm': {
                'model': SVC(random_state=42),
                'params': {
                    'C': [0.1, 1, 10],
                    'kernel': ['rbf', 'linear'],
                    'class_weight': ['balanced', None]
                }
            }
        }

        # K-Fold Cross-Validation for robust model training
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        for name, config in model_configs.items():
            print(f"Tuning {name}...")
            grid_search = GridSearchCV(
                estimator=config['model'], 
                param_grid=config['params'], 
                cv=cv, 
                scoring='f1_weighted', 
                n_jobs=-1
            )
            
            grid_search.fit(X_train_combined, y_train)
            
            # Store best model
            self.models[name] = grid_search.best_estimator_
            print(f"Best parameters for {name}: {grid_search.best_params_}")
        
        # LSTM model (keep existing implementation)
        self.models['lstm'] = self._create_lstm_model(X_train.shape[1])
        if name == 'lstm':
            X_reshaped = X_train_combined.reshape((X_train_combined.shape[0], 1, X_train_combined.shape[1]))
            self.models['lstm'].fit(X_reshaped, y_train, epochs=10, batch_size=32, validation_split=0.2)
        
        print("Advanced model training completed.")
    
    def _create_lstm_model(self, input_dim):
        model = Sequential([
            LSTM(64, input_shape=(1, input_dim)),
            Dense(32, activation='relu'),
            Dropout(0.2),
            Dense(1, activation='sigmoid')
        ])
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model
    def evaluate_models(self, X_test, y_test):
        print("evaluating the model...")
        results = {}
    
        # Ensure test features are processed similarly to training
        n_text_features = self.vectorizer.get_feature_names_out().shape[0]
        X_test_text = X_test[:, :n_text_features]
        X_test_other = X_test[:, n_text_features:]
        
        X_test_other = self.imputer.transform(X_test_other)
        X_test_other_scaled = self.scaler.transform(X_test_other)
        X_test_combined = np.hstack((X_test_text, X_test_other_scaled))
        
        for name, model in self.models.items():
            if name == 'lstm':
                X_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
                y_pred = (model.predict(X_reshaped) > 0.5).astype(int)
            else:
                y_pred = model.predict(X_test_combined)
            
            results[name] = {
                'classification_report': classification_report(y_test, y_pred),
                'confusion_matrix': confusion_matrix(y_test, y_pred),
                'accuracy': accuracy_score(y_test, y_pred)
            }
    
            # Ensemble Voting Classifier
        from sklearn.ensemble import VotingClassifier
        voting_models = [
            ('rf', self.models['random_forest']),
            ('lr', self.models['logistic_regression']),
            ('svm', self.models['svm'])
        ]
        voting_clf = VotingClassifier(estimators=voting_models, voting='hard')
        voting_clf.fit(X_test_combined, y_test)
        voting_pred = voting_clf.predict(X_test_combined)
        
        results['ensemble'] = {
            'classification_report': classification_report(y_test, voting_pred),
            'confusion_matrix': confusion_matrix(y_test, voting_pred),
            'accuracy': accuracy_score(y_test, voting_pred)
        }
        
        return results
    
    def predict(self, text):
        """Predict using the trained models with proper feature handling."""
        # Preprocess text
        processed_text = self.preprocess_text(text)
        
        # Extract TF-IDF features
        text_features = self.vectorizer.transform([processed_text]).toarray()
        
        # Extract other features in the same order as during training
        other_features = {}
        other_features.update(self.extract_linguistic_features(processed_text))
        other_features.update(self.extract_sentiment_features(processed_text))
        other_features.update(self.extract_structural_features(processed_text))
        other_features.update(self.extract_pos_features(processed_text))
        other_features.update(self.extract_readability_features(processed_text))
        
        # Convert to DataFrame with correct column order
        other_features_df = pd.DataFrame([other_features])
        other_features_df = other_features_df.reindex(columns=self.feature_names, fill_value=0)
        
        # Handle missing values and scale
        other_features_array = self.imputer.transform(other_features_df)
        scaled_features = self.scaler.transform(other_features_array)
        
        # Combine features
        all_features = np.hstack((text_features, scaled_features))
        
        # Make predictions
        predictions = {}
        for name, model in self.models.items():
            if name == 'lstm':
                # Reshape for LSTM
                reshaped_features = all_features.reshape(1, 1, -1)
                pred = model.predict(reshaped_features)
                predictions[name] = (pred > 0.5).astype(int).flatten()[0]
            else:
                predictions[name] = model.predict(all_features)[0]
        
        # Convert numeric predictions to labels
        labeled_predictions = {
            name: self.le.inverse_transform([pred])[0] 
            for name, pred in predictions.items()
        }
        
        return labeled_predictions

    def manual_testing(self):
        """Allow manual testing of the model with user input."""
        print("\n=== Fake News Detection Manual Testing ===")
        while True:
            user_input = input("\nPaste your news article (or type 'exit' to quit):\n")
            if user_input.lower() == 'exit':
                break
            
            try:
                predictions = self.predict(user_input)
                
                print("\nPredictions:")
                for model_name, prediction in predictions.items():
                    print(f"{model_name.replace('_', ' ').title()}: {prediction}")
                
                # Calculate consensus prediction
                prediction_values = list(predictions.values())
                most_common = max(set(prediction_values), key=prediction_values.count)
                consensus_ratio = prediction_values.count(most_common) / len(prediction_values)
                
                print(f"\nFinal Prediction: {most_common}")
                print(f"Confidence: {consensus_ratio:.1%}\n")
                
            except Exception as e:
                print(f"Error occurred: {str(e)}")
        

def main():
    try:
        # Load your dataset
        print("Loading dataset...")
        df = pd.read_csv('final_en.csv')
        print(f"Dataset loaded with {len(df)} rows")
        
        # Clean dataset
        print("Cleaning dataset by dropping unnecessary unnamed columns...")
        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
        
        # Initialize FakeNewsDetector
        print("Initializing FakeNewsDetector...")
        detector = FakeNewsDetector()

        # Prepare data
        print("Preparing data...")
        X, y = detector.prepare_data(df, 'text', 'label')
        print(f"Features shape: {X.shape}, Labels shape: {y.shape}")

        # Split data
        print("Splitting data into train and test sets...")
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Train models
        print("Training models...")
        detector.train_models(X_train, y_train, X_test)

        # Evaluate models
        print("\nEvaluating models...")
        results = detector.evaluate_models(X_test, y_test)
        
        # Print evaluation results
        for model_name, result in results.items():
            print(f"\n{model_name.upper()} Results:")
            print(result['classification_report'])
            print("\nConfusion Matrix:")
            print(result['confusion_matrix'])
            
        # Start manual testing
        print("\nStarting manual testing mode...")
        detector.manual_testing()
        
    except FileNotFoundError:
        print("Error: Could not find the news.csv file. Please ensure it exists in the correct directory.")
    except pd.errors.EmptyDataError:
        print("Error: The news.csv file is empty.")
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        raise

if __name__ == "__main__":
    main()